@misc{DOD,
  author = {Franco, Nicola Rares and Manzoni, Andrea and Zunino, Paolo and Hesthaven, Jan S.},
  title = {Deep orthogonal decomposition: a continuously adaptive data-driven approach to model order reduction},
  year = {2024},
  howpublished = {Preprint, arXiv:2404.18841 [math.NA] (2024)},
  url = {https://arxiv.org/abs/2404.18841},
  note = {arXiv:2404.18841}
}

@article{POD-DL-ROM,
   title={POD-DL-ROM: Enhancing deep learning-based reduced order models for nonlinear parametrized PDEs by proper orthogonal decomposition},
   volume={388},
   ISSN={0045-7825},
   url={http://dx.doi.org/10.1016/j.cma.2021.114181},
   DOI={10.1016/j.cma.2021.114181},
   journal={Computer Methods in Applied Mechanics and Engineering},
   publisher={Elsevier BV},
   author={Fresca, Stefania and Manzoni, Andrea},
   year={2022},
   month=jan, pages={114181} }

@misc{CoLoRA,
      title={CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations}, 
      author={Jules Berman and Benjamin Peherstorfer},
      year={2024},
      eprint={2402.14646},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.14646}, 
}

@misc{DL-ROM,
      title={A comprehensive deep learning-based approach to reduced order modeling of nonlinear time-dependent parametrized PDEs}, 
      author={Stefania Fresca and Luca Dede and Andrea Manzoni},
      year={2020},
      eprint={2001.04001},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2001.04001}, 
}

@book{quarteroni2015rb,
  title     = {Reduced Basis Methods for Partial Differential Equations},
  author    = {Quarteroni, Alfio and Manzoni, Andrea and Negri, Federico},
  year      = {2015},
  publisher = {Springer},
  doi       = {10.1007/978-3-319-15431-2}
}

@article{Existence_of_ReLU_NN,
  title = {Approximation capabilities of multilayer feedforward networks},
  journal = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251-257},
  year = {1991},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/0893-6080(91)90009-T},
  url = {https://www.sciencedirect.com/science/article/pii/089360809190009T},
  author = {Kurt Hornik},
  keywords = {Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.}
}